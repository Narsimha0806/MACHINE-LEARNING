# -*- coding: utf-8 -*-
"""LogisticRegression.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ggkFnStdXaBFesusztzg5somH7Mbk9fO
"""

from google.colab import files
uploaded = files.upload()

import pandas as pd
Iris_dataset = pd.read_csv("Iris.csv")
Iris_dataset

Iris_dataset.head()

import numpy as np
#here we are multiclass logistic regression rather than standard logistic regression because here we have three classes to predict .
class MultiClassLogisticRegressor:
    def __init__(self, lr=0.01, epochs=1000):
        self.lr = lr  # Learning rate--> learning rate is the step size for the model.
        self.epochs = epochs
        self.weights = None  # Model weights
        self.bias = None  # Model bias

    def _apply_softmax(self, z):  #here we use softmax function because it shows probability distribution over multiple classes.
        exp_values = np.exp(z - np.max(z, axis=1, keepdims=True))
        return exp_values / np.sum(exp_values, axis=1, keepdims=True)

    def fit(self, inputs, labels):     #fit method is used to train the model .
        num_samples, num_features = inputs.shape
        num_classes = len(np.unique(labels))  # Determine the number of unique classes

        self.weights = np.zeros((num_features, num_classes))
        self.bias = np.zeros((1, num_classes))

        one_hot_labels = np.eye(num_classes)[labels]  #one hot encode helps the model to understand the correct class.

        for _ in range(self.epochs):
            linear_model = np.dot(inputs, self.weights) + self.bias
            predictions = self._apply_softmax(linear_model) #here we are applying softmax function to get probabilities .

            dw = (1 / num_samples) * np.dot(inputs.T, (predictions - one_hot_labels))
            db = (1 / num_samples) * np.sum(predictions - one_hot_labels, axis=0)

            self.weights -= self.lr * dw
            self.bias -= self.lr * db

    def predict(self, new_data):  #predict method makes predictions using the trained model.
        linear_model = np.dot(new_data, self.weights) + self.bias
        probs = self._apply_softmax(linear_model)
        return np.argmax(probs, axis=1)  # Return the class with the highest probability

