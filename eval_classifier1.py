# -*- coding: utf-8 -*-
"""eval_classifier1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LskEQJjg2um8FZM0-Jg-fua9z5bMct8w
"""

from google.colab import files
uploaded = files.upload()

import pandas as pd
Iris_dataset = pd.read_csv("Iris.csv")
Iris_dataset

Iris_dataset.head()

import numpy as np
#here we are multiclass logistic regression rather than standard logistic regression because here we have three classes to predict .
class MultiClassLogisticRegressor:
    def __init__(self, lr=0.01, epochs=1000):
        self.lr = lr  # Learning rate--> learning rate is the step size for the model.
        self.epochs = epochs
        self.weights = None  # Model weights
        self.bias = None  # Model bias

    def _apply_softmax(self, z):  #here we use softmax function because it shows probability distribution over multiple classes.
        exp_values = np.exp(z - np.max(z, axis=1, keepdims=True))
        return exp_values / np.sum(exp_values, axis=1, keepdims=True)

    def fit(self, inputs, labels):     #fit method is used to train the model .
        num_samples, num_features = inputs.shape
        num_classes = len(np.unique(labels))  # Determine the number of unique classes

        self.weights = np.zeros((num_features, num_classes))
        self.bias = np.zeros((1, num_classes))

        one_hot_labels = np.eye(num_classes)[labels]  #one hot encode helps the model to understand the correct class.

        for _ in range(self.epochs):
            linear_model = np.dot(inputs, self.weights) + self.bias
            predictions = self._apply_softmax(linear_model) #here we are applying softmax function to get probabilities .

            dw = (1 / num_samples) * np.dot(inputs.T, (predictions - one_hot_labels))
            db = (1 / num_samples) * np.sum(predictions - one_hot_labels, axis=0)

            self.weights -= self.lr * dw
            self.bias -= self.lr * db

    def predict(self, new_data):  #predict method makes predictions using the trained model.
        linear_model = np.dot(new_data, self.weights) + self.bias
        probs = self._apply_softmax(linear_model)
        return np.argmax(probs, axis=1)  # Return the class with the highest probability

from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
import numpy as np

Iris_dataset= load_iris()
petal_length_width_features = Iris_dataset.data[:, 2:4] #we use the petallength and petalwidth as features
class_labels = Iris_dataset.target  #class labels as target

# here we Split the dataset into training and testing sets
train_features, test_features, train_labels, test_labels = train_test_split(petal_length_width_features, class_labels, test_size=0.1, random_state=42)

log_reg_model = MultiClassLogisticRegressor(lr=0.01, epochs=1000)
log_reg_model.fit(train_features, train_labels)

np.save('trained_weights_petal.npy', log_reg_model.weights) #save weights
np.save('trained_bias_petal.npy', log_reg_model.bias) #save bias

from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from mlxtend.plotting import plot_decision_regions
import numpy as np
import matplotlib.pyplot as plt

Iris_dataset = load_iris()
petal__length_width_features = Iris_dataset.data[:, 2:4]  # here we Use petal length and petalwidth as features
target_classes = Iris_dataset.target  # Class labels as target

# here we Split the dataset into training and testing
train_data, test_data, train_labels, test_labels = train_test_split(petal_length_width_features, target_classes, test_size=0.1, random_state=42)

loaded_weights = np.load('trained_weights_petal.npy') #saved weights and bias are loaded here .
loaded_bias = np.load('trained_bias_petal.npy')

classifier_model = MultiClassLogisticRegressor()
classifier_model.weights = loaded_weights
classifier_model.bias = loaded_bias

predicted_labels = classifier_model.predict(test_data)

model_accuracy = np.mean(predicted_labels == test_labels)
print(f"Petal_length_width_features Accuracy: {model_accuracy}")

plt.figure(figsize=(6, 4))
plot_decision_regions(test_data, test_labels, clf=classifier_model, legend=1)

plt.title("Petal Length/Width", fontsize=12, fontweight='bold')
plt.xlabel("Petal Length", fontsize=10)
plt.ylabel("Petal Width", fontsize=10)
plt.grid(True)
plt.tight_layout()
plt.show()