# -*- coding: utf-8 -*-
"""train_classifier3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VeEqWBXKjzuQMR-e8O_OG9FwxMNOgUo5
"""

from google.colab import files
uploaded = files.upload()

import pandas as pd
Iris_dataset = pd.read_csv("Iris.csv")
Iris_dataset

Iris_dataset.head()

import numpy as np
#here we are multiclass logistic regression rather than standard logistic regression because here we have three classes to predict .
class MultiClassLogisticRegressor:
    def __init__(self, lr=0.01, epochs=1000):
        self.lr = lr  # Learning rate--> learning rate is the step size for the model.
        self.epochs = epochs
        self.weights = None  # Model weights
        self.bias = None  # Model bias

    def _apply_softmax(self, z):  #here we use softmax function because it shows probability distribution over multiple classes.
        exp_values = np.exp(z - np.max(z, axis=1, keepdims=True))
        return exp_values / np.sum(exp_values, axis=1, keepdims=True)

    def fit(self, inputs, labels):     #fit method is used to train the model .
        num_samples, num_features = inputs.shape
        num_classes = len(np.unique(labels))  # Determine the number of unique classes

        self.weights = np.zeros((num_features, num_classes))
        self.bias = np.zeros((1, num_classes))

        one_hot_labels = np.eye(num_classes)[labels]  #one hot encode helps the model to understand the correct class.

        for _ in range(self.epochs):
            linear_model = np.dot(inputs, self.weights) + self.bias
            predictions = self._apply_softmax(linear_model) #here we are applying softmax function to get probabilities .

            dw = (1 / num_samples) * np.dot(inputs.T, (predictions - one_hot_labels))
            db = (1 / num_samples) * np.sum(predictions - one_hot_labels, axis=0)

            self.weights -= self.lr * dw
            self.bias -= self.lr * db

    def predict(self, new_data):  #predict method makes predictions using the trained model.
        linear_model = np.dot(new_data, self.weights) + self.bias
        probs = self._apply_softmax(linear_model)
        return np.argmax(probs, axis=1)  # Return the class with the highest probability

from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
import numpy as np

Iris_dataset= load_iris()
all_length_width_features = Iris_dataset.data #we use all features sepallength , sepalwidth , petallength , petalwidth
class_labels = Iris_dataset.target  #class labels as target

# here we Split the dataset into training and testing sets
train_features, test_features, train_labels, test_labels = train_test_split(all_length_width_features, class_labels, test_size=0.1, random_state=42)

log_reg_model = MultiClassLogisticRegressor(lr=0.01, epochs=1000)
log_reg_model.fit(train_features, train_labels)

np.save('trained_weights.npy', log_reg_model.weights) #save weights
np.save('trained_bias.npy', log_reg_model.bias) #save bias

